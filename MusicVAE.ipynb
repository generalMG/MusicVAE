{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicVAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4A/bc8aeQMXZy++3wRCyh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/generalMG/MusicVAE/blob/main/MusicVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import ALL required packages and libraries"
      ],
      "metadata": {
        "id": "QUmgexxC8k-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onP9VRLnmCY4"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "BASE_DIR = \"gs://download.magenta.tensorflow.org/models/music_vae/colab2\"\n",
        "print(\"Dependencies installation...\")\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "!pip install -U -q magenta\n",
        "!gsutil -q -m cp gs://magentadata/models/music_vae/groovae_colab/*wav .\n",
        "!gsutil -q -m cp gs://magentadata/models/music_vae/checkpoints/*.tar .\n",
        "!gsutil -q -m cp -R gs://download.magenta.tensorflow.org/models/music_vae/colab2/checkpoints/mel_2bar_big.ckpt.* /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "\n",
        "ctypes.util.find_library = proxy_find_library"
      ],
      "metadata": {
        "id": "j8PG-gobnwyM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import magenta\n",
        "import magenta.music as mm\n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae.trained_model import TrainedModel\n",
        "import numpy as np\n",
        "import os\n",
        "import note_seq\n",
        "import tensorflow\n",
        "print(magenta.__version__)\n",
        "print(tensorflow.__version__)\n",
        "import tensorflow_datasets as tfds\n",
        "def quantize(s, steps_per_quarter=4):\n",
        "  return note_seq.sequences_lib.quantize_note_sequence(s,steps_per_quarter)\n",
        "def set_to_drums(ns):\n",
        "  for n in ns.notes:\n",
        "    n.instrument=9\n",
        "    n.is_drum = True\n",
        "\n",
        "def is_4_4(s):\n",
        "  ts = s.time_signatures[0]\n",
        "  return (ts.numerator == 4 and ts.denominator ==4)\n",
        "\n",
        "dataset_2bar = tfds.as_numpy(tfds.load(\n",
        "    name=\"groove/2bar-midionly\",\n",
        "    split=tfds.Split.VALIDATION,\n",
        "    try_gcs=True))\n",
        "\n",
        "dev_sequences = [quantize(note_seq.midi_to_note_sequence(features[\"midi\"])) for features in dataset_2bar]\n",
        "_ = [set_to_drums(s) for s in dev_sequences]\n",
        "dev_sequences = [s for s in dev_sequences if is_4_4(s) and len(s.notes) > 0 and s.notes[-1].quantized_end_step > note_seq.steps_per_bar_in_quantized_sequence(s)]\n",
        "\n",
        "dataset_4bar = tfds.as_numpy(tfds.load(\n",
        "    name=\"groove/4bar-midionly\",\n",
        "    split=tfds.Split.VALIDATION,\n",
        "    try_gcs=True))\n",
        "\n",
        "dev_sequences_4bar = [quantize(note_seq.midi_to_note_sequence(features[\"midi\"])) for features in dataset_4bar]\n",
        "_ = [set_to_drums(s) for s in dev_sequences_4bar]\n",
        "dev_sequences_4bar = [s for s in dev_sequences_4bar if is_4_4(s) and len(s.notes) > 0 and s.notes[-1].quantized_end_step > note_seq.steps_per_bar_in_quantized_sequence(s)]\n",
        "\n"
      ],
      "metadata": {
        "id": "9m4V8mc_ohAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play(note_sequence):\n",
        "  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n",
        "\n",
        "def interpolate(model, start_seq, end_seq, num_steps, max_length=32,\n",
        "                assert_same_length=True, temperature=0.5, individual_duration=4.0):\n",
        "  note_sequences = model.interpolate(start_seq, end_seq, num_steps=num_steps, length=max_length, temperature=temperature, assert_same_length=assert_same_length)\n",
        "  print('Start Seq Reconstruction')\n",
        "  play(note_sequences[0])\n",
        "  print('End Seq Reconstruction')\n",
        "  play(note_sequences[-1])\n",
        "  print('Mean Sequence')\n",
        "  play(note_sequences[num_steps // 2])\n",
        "  print('Start -> End Interpolation')\n",
        "  interp_seq = mm.sequences_lib.concatenate_sequences(note_sequences, [individual_duration] * len(note_sequences))\n",
        "  play(interp_seq)\n",
        "  mm.plot_sequence(interp_seq)\n",
        "  return interp_seq if num_steps > 3 else note_sequences[num_steps // 2]\n",
        "def download(note_sequence, filename):\n",
        "  mm.sequence_proto_to_midi_file(note_sequence, filename)\n",
        "  files.download(filename)"
      ],
      "metadata": {
        "id": "e2UFE83FpJoo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to init. models click the run botton of the cell below.\n"
      ],
      "metadata": {
        "id": "mjurql_a8uqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Music VAE...\")\n",
        "music_vae = {}\n",
        "cat_mel_2bar_big = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
        "cat_drums_2bar_small = configs.CONFIG_MAP['cat-drums_2bar_small']\n",
        "nade_drums_2bar_reduced = configs.CONFIG_MAP['nade-drums_2bar_reduced']\n",
        "nade_drums_2bar_full = configs.CONFIG_MAP['nade-drums_2bar_full']\n",
        "music_vae['cat_mel_2bar_big'] = TrainedModel(cat_mel_2bar_big, batch_size=4, checkpoint_dir_or_path='/content/mel_2bar_big.ckpt')\n",
        "music_vae['drums_2bar_oh_lokl'] = TrainedModel(cat_drums_2bar_small, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/drums_2bar_small.lokl.ckpt')\n",
        "music_vae['drums_2bar_oh_hikl'] = TrainedModel(cat_drums_2bar_small, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/drums_2bar_small.hikl.ckpt')\n",
        "music_vae['drums_2bar_nade_reduced'] = TrainedModel(nade_drums_2bar_reduced, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/drums_2bar_nade.reduced.ckpt')\n",
        "music_vae['drums_2bar_nade_full'] = TrainedModel(nade_drums_2bar_full, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/drums_2bar_nade.full.ckpt')\n"
      ],
      "metadata": {
        "id": "q8Ug6r2-2TYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Samples"
      ],
      "metadata": {
        "id": "Ub1qA4ba7RLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'drums_2bar_oh_lokl' #@param [\"cat_mel_2bar_big\",\"drums_2bar_oh_lokl\", \"drums_2bar_oh_hikl\", \"drums_2bar_nade_reduced\", \"drums_2bar_nade_full\"]\n",
        "temperature = 0.5 #@param {type:\"slider\", min:0.1, max:2.0, step:0.1}\n",
        "n = 2 #@param {type:\"slider\", min:2, max:10, step:1}\n",
        "length = 10 #@param {type:\"slider\", min:5, max:80, step:1}\n",
        "generated_sequences = music_vae[model].sample(n=n, length=length, temperature=temperature)\n",
        "for i, ns in enumerate(generated_sequences):\n",
        "  note_seq.plot_sequence(ns)\n",
        "  note_seq.play_sequence(ns, synth=note_seq.fluidsynth)"
      ],
      "metadata": {
        "id": "rswfr8QNIvgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell below downloads note sequence as MIDI files in your LOCAL SYSTEM\n"
      ],
      "metadata": {
        "id": "7geioYMp9D8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download MIDI files to local system\n",
        "for i, ns in enumerate(generated_sequences):\n",
        "  download(ns, '%s_sample_%d.mid' % (model, i))"
      ],
      "metadata": {
        "id": "khqtjP8j5QkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please Upload saved MIDI files back again from Local System to COLAB\\\n",
        "NB: I was trying to directly implement MIDI files/note sequence to the model without back and forth downloading/uploading process, but I am lacking a bit deeper knowledge of the note sequence/midi"
      ],
      "metadata": {
        "id": "8rSP50fn9Lhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload saved MIDI files from Local System to COLAB\n",
        "## NB: I was trying to directly implement MIDI files/note sequence to the model without back and forth downloading/uploading process, but I am lacking a bit deeper knowledge of the note sequence/midi\n",
        "input_drums_midi_data = files.upload().values() or input_drums_midi_data"
      ],
      "metadata": {
        "id": "tW_EUQdZvc6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_drums_midi_data = [\n",
        "    tensorflow.io.gfile.GFile(fn, mode='rb').read()\n",
        "    for fn in sorted(tensorflow.io.gfile.glob(BASE_DIR + '/midi/drums_2bar*.mid'))]"
      ],
      "metadata": {
        "id": "r5kORyaLtSPq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if all steps above were followed correctly there shouldn't be a problem with this particular step\n",
        "drums_input_seqs = [mm.midi_to_sequence_proto(m) for m in input_drums_midi_data]\n",
        "extracted_beats = []\n",
        "# Config shall be changed according to the music_vae model\n",
        "config = cat_drums_2bar_small # Change here from the cells above or copy paste: cat_mel_2bar_big, cat_drums_2bar_small, nade_drums_2bar_reduced, nade_drums_2bar_full\n",
        "for ns in drums_input_seqs:\n",
        "  extracted_beats.extend(config.data_converter.from_tensors(\n",
        "      config.data_converter.to_tensors(ns)[1]))\n",
        "for i, ns in enumerate(extracted_beats):\n",
        "  print(\"Beat\", i)\n",
        "  #play(ns)\n",
        "  note_seq.plot_sequence(ns)\n",
        "  note_seq.play_sequence(ns, synth=note_seq.fluidsynth)"
      ],
      "metadata": {
        "id": "tUSiG06YtjY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Interpolations"
      ],
      "metadata": {
        "id": "0JEclW6r7KS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interpolate between 2 beats, selected from those in the previous cell.\n",
        "model = \"drums_2bar_oh_lokl\" #@param [\"cat_mel_2bar_big\",\"drums_2bar_oh_lokl\", \"drums_2bar_oh_hikl\", \"drums_2bar_nade_reduced\", \"drums_2bar_nade_full\"]\n",
        "start_beat = 0 #@param {type:\"integer\"}\n",
        "end_beat = 1 #@param {type:\"integer\"}\n",
        "start_beat = extracted_beats[start_beat]\n",
        "end_beat = extracted_beats[end_beat]\n",
        "\n",
        "temperature = 0.5 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
        "num_steps = 13 #@param {type:\"integer\"}\n",
        "\n",
        "drums_interp = interpolate(music_vae[model], start_beat, end_beat, num_steps=num_steps, temperature=temperature)"
      ],
      "metadata": {
        "id": "03uicgCz6kl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}